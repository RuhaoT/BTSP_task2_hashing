from .utils import *
class LSH(object):
	def __init__(self, data, hash_length):
		"""
		data: Nxd matrix
		hash_length: scalar
		sampling_ratio: fraction of input dims to sample from when producing a hash
		(ratio of PNs that each KC samples from)
		embedding_size: dimensionality of projection space, m
		"""
		self.hash_length = hash_length
		self.L2_distance = []
		self.data = data
		self.data = data
		self.weights = torch.rand((data.shape[1], hash_length))
		self.mem = self.data @ self.weights
		self.hashes = self.mem
		self.maxl1distance = 2 * self.hash_length

	def query(self, qidx, nnn, not_olap=False):
		"""
		calculate the sample-similarity from the hashing pooling generated by the whole datasets.
		:param qidx: querying index from datsets
		:param nnn: calculate the nnn of nearest neighboor points
		:param not_olap:
		:return: NNs: represent the index of  #nnn neareast samples
		"""
		L1_distances = sample_dist(self.hashes[qidx, :], self.hashes)
		# np.sum(np.bitwise_xor(self.hashes[qidx,:],self.hashes),axis=1)
		nnn = min(self.hashes.shape[0], nnn)
		if not_olap:
			no_overlaps = np.sum(L1_distances == self.maxl1distance)
			return no_overlaps
		NNs = L1_distances.argsort(axis =0)
		NNs = NNs[(NNs != qidx)][:nnn]
		# print(L1_distances[NNs]) #an interesting property of this hash is that the L1 distances are always even
		return NNs

	def true_nns(self, qidx, nnn):
		"""
		calculate the sample-based similarity
		:param qidx: querying sample
		:param nnn:
		:return: the index of nearest samples
		"""
		sample = self.data[qidx, :]
		L1_distances = sample_dist(sample, self.data)
		tnns = L1_distances.argsort()[:nnn + 1]
		tnns = tnns[(tnns != qidx)]
		if nnn < self.data.shape[0]:
			assert len(tnns) == nnn, 'nnn={}'.format(nnn)
		return tnns

	def construct_true_nns(self, indices, nnn):
		all_NNs = np.zeros((len(indices), nnn))
		for idx1, idx2 in enumerate(indices):
			all_NNs[idx1, :] = self.true_nns(idx2, nnn)
		return all_NNs

	def AP(self, predictions, truth):
		assert len(predictions) == len(truth) or len(predictions) == self.hashes.shape[0]
		# removed conversion to list in next line:
		# for the k elements, it records the ratio of the equal number between the neareast data calculated bv raw data and
		# the nearest data calculated by projected data over the first k candidates.
		precisions = [len((set(predictions[:idx]).intersection(set(truth[:idx])))) / idx for \
					  idx in range(1, len(truth) + 1)]
		return np.mean(precisions)

	def PR(self, qidx, truth, atindices):
		"""truth should be a set"""
		L1_distances = sample_dist(self.hashes[qidx, :], self.hashes)
		NNs = L1_distances.argsort(axis=0)
		NNs = NNs[(NNs != qidx)]
		# predictions=NNs
		recalls = np.arange(1, len(truth) + 1)
		all_recalls = [len(set(NNs[:idx]) & truth) for idx in atindices]
		precisions = [recall / (idx + 1) for idx, recall in zip(atindices, all_recalls)]
		# this_pr=odict({l:(p,r) for l,p,r in zip(atL1,precisions,recalls)})
		return precisions, all_recalls  # (precisions,all_recalls)

	def ROC(self, qidx, truth, atindices):
		"""x: False positive rate, y: True positive rate, truth should be a set"""
		L1_distances = sample_dist(self.hashes[qidx, :], self.hashes)
		NNs = L1_distances.argsort()
		NNs = NNs[(NNs != qidx)]
		x, y = [], []
		for idx in atindices:
			ntruepos = len((set(NNs[:idx]) & truth))  # number of positives correctly classified
			nfalseneg = idx - ntruepos  # number of negatives incorrectly classified
			tpr = ntruepos / len(truth)  # positives correctly classified / total positives
			fpr = nfalseneg / (len(NNs) - len(truth))  # negatives incorrectly classified / total negatives
			x.append(fpr)
			y.append(tpr)
		return x, y

	def findmAP(self, nnn, n_points):
		sample_indices = np.random.choice(self.data.shape[0], n_points) # sample n_points from the clustered data
		# calculate the nearest n_points based on input_data distribution in advance
		all_NNs = self.construct_true_nns(sample_indices, nnn) # all_NNs: return a quering_sample_num * candicate_num matrix
		self.allAPs = []
		for eidx, didx in enumerate(sample_indices):
			# eidx: enumeration id, didx: index of sample in self.data
			this_nns = self.query(didx, nnn)
			# print(len(this_nns))
			this_AP = self.AP(list(this_nns), list(all_NNs[eidx, :]))
			# print(this_AP)
			self.allAPs.append(this_AP)
		return np.mean(self.allAPs)

	def test_true_nns(self, sample, nnn):
		"""
		calculate the sample-based similarity
		:param qidx: querying sample
		:param nnn:
		:return: the index of nearest samples
		"""
		tnns = np.sum((self.data - sample) ** 2, axis=1).argsort()[:nnn]
		# tnns = tnns[(tnns != qidx)]
		# if nnn < self.data.shape[0]:
		# 	assert len(tnns) == nnn, 'nnn={}'.format(nnn)
		return tnns

	def feedforward(self, x):
		samples = x.reshape(x.shape[0],-1)
		activations = self.weights.transpose() @ samples
		npts = np.sort(activations, axis=0)[-self.hash_length]
		return activations>npts

	def test_query(self, sample, nnn, not_olap=False):
		"""
		calculate the sample-similarity from the hashing pooling generated by the whole datasets.
		:param qidx: querying index from datsets
		:param nnn: calculate the nnn of nearest neighboor points
		:param not_olap:
		:return: NNs: represent the index of  #nnn neareast samples
		"""
		query_hash = self.feedforward(sample)
		L1_distances = sample_dist(query_hash, self.hashes)
		# L2_distances = np.sum((np.float32(query_hash.flatten()) - np.float32(self.hashes))**2, axis=1)# ^:XOR operations
		# np.sum(np.bitwise_xor(self.hashes[qidx,:],self.hashes),axis=1)
		nnn = min(self.hashes.shape[0], nnn)

		NNs = L1_distances.argsort(axis =0)
		NNs = NNs[:nnn]

		return NNs

	def findmAP(self, nnn, n_points):
		# start = np.random.randint(low=0, high=self.data.shape[0] - n_points)
		sample_indices = np.random.choice(self.data.shape[0], n_points)
		all_NNs = self.construct_true_nns(sample_indices, nnn)
		self.allAPs = []
		for eidx, didx in enumerate(sample_indices):
			# eidx: enumeration id, didx: index of sample in self.data
			this_nns = self.query(didx, nnn)
			# print(len(this_nns))
			this_AP = self.AP(list(this_nns), list(all_NNs[eidx, :]))
			# print(this_AP)
			self.allAPs.append(this_AP)
		return np.mean(self.allAPs)


	def test_findmAP(self, nnn, test_data):
		# todo: obtain an array recording the nearest points for each query sample; return all_NNs
		all_NNs = np.zeros((test_data.shape[0], nnn))
		for idx1, idx2 in enumerate(test_data):
			all_NNs[idx1, :] = self.test_true_nns( test_data[idx1], nnn)
		self.allAPs = []
		self.L2_distance = []
		import copy
		for eidx, didx in enumerate(test_data):
			# todo: obtain the projected results
			this_nns = self.test_query(test_data[eidx], nnn)

			query_mem = test_data[eidx].reshape(1, -1) @ self.weights
			tmp = (self.mem[this_nns] - query_mem)**2
			self.L2_distance.append(copy.deepcopy(tmp.mean()))
			# todo: calculate the similarities
			this_AP = self.AP(list(this_nns), list(all_NNs[eidx, :]))
			self.allAPs.append(this_AP)
		return np.mean(self.allAPs), np.mean(self.L2_distance)